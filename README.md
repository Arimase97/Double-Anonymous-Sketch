# Double-Anonymous Sketch: Achieving Fairness for Global Top-K Frequent Items

This repository contains all the related code of our paper "Double-Anonymous Sketch:  Achieving Fairness for Global Top-K Frequent Items". 

## Introduction

Finding top-K frequent items has been a hot topic in data stream processing in recent years, which has a wide range of applications. However, the body of existing sketch algorithms focuses on finding local top-K in a single data stream, while only a few focus on finding global top-K in distributed data streams, which is also important in the era of big data. We find that deploying existing sketch algorithms is often unfair under global scenarios, which will degrade the accuracy of global top-K. To achieve fairness, we propose a new sketch framework, called the Double-Anonymous sketch. The process of finding global top-K items is similar to that of paper reviewing and democratic elections. In these scenarios, double-anonymity is often an effective strategy to achieve fairness. We also propose two techniques, hot panning, and early freezing, to rise accuracy further. We theoretically prove that the Double-Anonymous sketch achieves fairness and derives an error bound strictly tighter than the well-known Count-Min sketch bound. We perform extensive experiments to verify fairness in the distributed scenario. The experimental results show that the Double-Anonymous sketchâ€™s error is up to 129 times (60 times on average) smaller than the state-of-the-art.


## About This Repository

This repository contains all the related code for our experiments in the `src` directory. We have implemented four Double-Anonymous applications for both finding local top-K items and finding global top-K items. Source codes for finding local top-K are located in `local_topk/`, and source codes for on finding global top-K items are located in `global_topk/`. We also compare our results with state-of-the-art approaches.



## Requirements

- Ubuntu 16.04.6 LTS
- g++ >= 7.5.0



## Usage

### Datasets

We conduct experiments on four datasets: CAIDA [1], Webpage [2], Network [3], and Synthetic datasets. CAIDA, Webpage and Network datasets are open-sourced real-world datasets, while Synthetic datasets are generated by Zip-f distributions. You can download CAIDA, Webpage and Network datasets in their respective websites:

```
CAIDA: https://www.caida.org/data/overview/
Webpage: http://fimi.ua.ac.be/data/webdocs.dat.gz
Network: http://snap.stanford.edu/data/
```
We also upload and share Synthetic Dataset anonymously: `https://1drv.ms/u/s!AjAJpP5s7taYarAZiixWbjs7Kfs?e=xGTA6y`.

#### Dataset Setup

First, download the related datasets and put them into the `data/` folder.

- For CAIDA, Webpage, Network, and Synthetic datasets: we have implemented related functions for dataset parsing.

- Rename CAIDA, Webpage, Network and Synthetic datasets as `caida.dat`, `webdocs_form00.dat`, `net.dat` and `009.dat` respectively.



### How to Build

You can simply build all the programs for our experiment with the following commands:

```
cd local_topk/
make
cd ../
cd global_topk/
make
```


### How to Reproduce Our Experiments

Once successfully building all the programs, you need to create your own test cases to specify your desired experimental settings in a file (e.g., config.txt). All the executables accept one parameter called `file_name` and read your constructed configuration in `file_name`. We have provided sample files in `local_topk/config.txt` and `global_topk/config*.txt`. The contents of `local_topk/config.txt` and their corresponding explanations are as follows:

```
5 0 normal_test.txt           # Number_of_test_cases;   Dataset_id: 0:CAIDA, 1:Webpage, 2:Net, 3:Synthetic;   output_file_name
100 55 1000 8             # test cases 1:   Memory (KB);   $M_{top-K}/M$;   value_of_K;   $lambda$:number_of_cells_in_each_bucket
200 55 1000 8
300 55 1000 8
400 55 1000 8
500 55 1000 8
```

While the contents of `global_topk/config.txt` are something like the followings:

```
5 ../data_distributed/ balanced.txt          # Number_of_test_cases;   Input data path;   output_file_name
10000 10 1000 55 8          # test cases 1:   Memory (Byte per sketch);   $N$: number of distributed data streams;     $M_{top-K}/M$;   value_of_K;   $lambda$:number_of_cells_in_each_bucket
20000 10 1000 55 8
30000 10 1000 55 8
40000 10 1000 55 8
50000 10 1000 55 8
```




After that, create a folder called `./data_distributed`. Then you should just run `local_topk/run.sh` or `global_topk/run_balanced.sh` or `global_topk/run_skewed.sh` to execute all the experiments. All the programs will run automatically and produce the results in a file specified in your configuration.


## References
[1] The caida anonymized 2016 internet traces. http://www.caida.org/data/overview/.

[2] Real-life transactional dataset. http://fimi.ua.ac.be/data/.

[3] The Network dataset Internet Traces. http://snap.stanford.edu/data/.
